# AI Summer Project 2025: Investigating Grokking Acceleration in Deep Neural Networks

## Project Overview
This project explores the effects of **gradient filtering** on **grokking acceleration** in deep neural networks. The goal is to analyze how filtering impacts **learning dynamics, feature rank evolution, and generalization**.

## Research Questions
- How does **gradient filtering** affect **grokking acceleration**?
- What impact does it have on **feature rank evolution**?
- Can it **enhance generalization** in deep learning models?

## Methodology
- **Model:** Multi-Layer Perceptron (MLP)
- **Dataset:** Modular arithmetic dataset
- **Key Experiment:** Compare a **baseline model** vs. a **filtered model** using accuracy analysis and feature rank evolution.

## Project Structure

main.ipynb â€“ Jupyter Notebook for running experiments and visualization
models/ â€“ Jupyter Notebook for saving and loading trained models
train/ â€“ Jupyter Notebook for training models
utils/ â€“ Python utilities for data processing and training
__pycache__/ â€“ Python cache files


## Saved Model Checkpoints

results_1500_epochs.pth â€“ Model trained for 1500 epochs
results_500_epochs.pth â€“ Model trained for 500 epochs
results_advanced.pth â€“ Advanced training results
results.pth â€“ General model results
mlp_model.pth â€“ Trained MLP model
loss_history.pth â€“ Training loss history


## Key Findings
- **Filtered models learn slower** but exhibit **different feature rank evolution**.
- Feature rank **stabilization** might **affect generalization**.
- **Gradient filtering modifies** how networks learn, requiring further study.

## Future Work
- Implement **adaptive filtering strategies**.
- Extend experiments to **other neural architectures**.
- Investigate the link between **rank stability and generalization**.

## Contact
ðŸ“§ [Your Email]  
ðŸ”— [Your LinkedIn (if applicable)]  

